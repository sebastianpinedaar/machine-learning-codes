{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Tutorial 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "np.set_printoptions(precision=4, floatmode='fixed', suppress=True, sign=' ', \n",
    "                    formatter={'int': lambda x: \"{:> }\".format(x)})\n",
    "\n",
    "D = np.array([[1,0,2,+1],[1,2,0,-1],[1,2,2,+1],[1,1,1,-1]])\n",
    "X = D[:,:-1]\n",
    "Y = D[:, -1]\n",
    "N, M = X.shape\n",
    "beta0 = np.zeros((M)) #+ [-3,-1,2]\n",
    "alpha0= np.zeros((N))\n",
    "lam=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient descent\n",
    "\n",
    "We vectorize the sum $\\sum_\\Omega y_nx_n$ with  $\\Omega = \\{n\\in\\{1,\\ldots N\\} \\mid y_n (\\beta_0 + \\beta^T x_n)<1\\}$ via masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:  0  mask: [ 1  1  1  1]  Z:[ 0  1 -3]  B: [ 0.0000 -0.1250  0.3750]  XB: [ 0.7500 -0.2500  0.5000  0.2500]\n",
      "it:  1  mask: [ 1  1  1  1]  Z:[ 0  1 -3]  B: [ 0.0000 -0.1875  0.5625]  XB: [ 1.1250 -0.3750  0.7500  0.3750]\n"
     ]
    }
   ],
   "source": [
    "def grad(X, Y, beta):\n",
    "    mask = Y*(X@beta)<1\n",
    "    x_masked = X[mask]\n",
    "    y_masked = Y[mask]\n",
    "    Z = -x_masked.T @ y_masked\n",
    "    return Z/len(X) + lam*beta, mask, Z\n",
    "\n",
    "def GD(X, Y, beta0, lam=1, eta=0.5, maxiter=2):\n",
    "    beta = beta0.copy()\n",
    "    for t in range(maxiter):\n",
    "        dbeta, mask, Z = grad(X, Y, beta)\n",
    "        beta -= eta*dbeta \n",
    "        print(F\"it: {t:2d}  mask: {mask.astype(int)}  Z:{Z}  B: {beta}  XB: {X@beta}\")\n",
    "\n",
    "GD(X, Y, beta0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:  0  mask: [ 1  1]  Z:[ 0  2 -2]  B: [ 0.0000 -0.7071  0.7071]  XB: [ 1.4142 -1.4142  0.0000  0.0000]\n",
      "it:  0  mask: [ 1  1]  Z:[ 0 -1 -1]  B: [ 0.0000 -0.1036  0.6036]  XB: [ 1.2071 -0.2071  1.0000  0.5000]\n",
      "it:  1  mask: [ 0  1]  Z:[ 1  2  0]  B: [-0.1667 -0.4024  0.4024]  XB: [ 0.6381 -0.9714 -0.1667 -0.1667]\n",
      "it:  1  mask: [ 1  1]  Z:[ 0 -1 -1]  B: [-0.1250 -0.1768  0.4268]  XB: [ 0.7286 -0.4786  0.3750  0.1250]\n"
     ]
    }
   ],
   "source": [
    "def pegasos(X, Y, beta0, lam=1, k=2, maxiter=2):\n",
    "    beta = beta0.copy()\n",
    "    t = 1\n",
    "    for it in range(maxiter):\n",
    "        nbatches = int(len(X)/k)\n",
    "        x_batches = np.array_split(X, nbatches)\n",
    "        y_batches = np.array_split(Y, nbatches)\n",
    "        for x,y in zip(x_batches, y_batches):\n",
    "            dbeta, mask, Z = grad(x,y,beta)\n",
    "            beta -= (lam/t)*dbeta\n",
    "            t += 1\n",
    "            beta *= min(1, 1/(np.sqrt(lam)*norm(beta)) )\n",
    "            print(F\"it: {it:2d}  mask: {mask.astype(int)}  Z:{Z}  B: {beta}  XB: {X@beta}\")\n",
    "\n",
    "pegasos(X, Y, beta0, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dual Coordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:  0  A: [ 0.2000  0.0000  0.0000  0.0000]  B: [ 0.2000  0.0000  0.4000]  XB: [ 1.0000  0.2000  1.0000  0.6000]\n",
      "it:  0  A: [ 0.2000  0.2400  0.0000  0.0000]  B: [-0.0400 -0.4800  0.4000]  XB: [ 0.7600 -1.0000 -0.2000 -0.1200]\n",
      "it:  0  A: [ 0.2000  0.2400  0.1333  0.0000]  B: [ 0.0933 -0.2133  0.6667]  XB: [ 1.4267 -0.3333  1.0000  0.5467]\n",
      "it:  0  A: [ 0.2000  0.2400  0.1333  0.2500]  B: [-0.1567 -0.4633  0.4167]  XB: [ 0.6767 -1.0833 -0.2500 -0.2033]\n",
      "it:  1  A: [ 0.2500  0.2400  0.1333  0.2500]  B: [-0.1067 -0.4633  0.5167]  XB: [ 0.9267 -1.0333  0.0000 -0.0533]\n",
      "it:  1  A: [ 0.2500  0.2333  0.1333  0.2500]  B: [-0.1000 -0.4500  0.5167]  XB: [ 0.9333 -1.0000  0.0333 -0.0333]\n",
      "it:  1  A: [ 0.2500  0.2333  0.2407  0.2500]  B: [ 0.0074 -0.2352  0.7315]  XB: [ 1.4704 -0.4630  1.0000  0.5037]\n",
      "it:  1  A: [ 0.2500  0.2333  0.2407  0.2500]  B: [ 0.0074 -0.2352  0.7315]  XB: [ 1.4704 -0.4630  1.0000  0.5037]\n"
     ]
    }
   ],
   "source": [
    "def dcd(X, Y, beta0, alpha0, lam=1, maxiter=2):\n",
    "    beta = beta0.copy()\n",
    "    alpha = alpha0.copy()\n",
    "    Q = (X*Y[:,None]) @ (X*Y[:,None]).T\n",
    "    D = np.diag(Q)\n",
    "    C = 1/(len(X)*lam)\n",
    "    for it in range(maxiter):\n",
    "        for n,(a,x,y,q) in enumerate(zip(alpha, X, Y, D)):\n",
    "            yhat = x @ beta\n",
    "            dalpha = (y*yhat -1)/q\n",
    "            alpha[n] = np.clip(a - dalpha, 0, C) \n",
    "            beta += (alpha[n] - a)*y*x\n",
    "            print(F\"it: {it:2d}  A: {alpha}  B: {beta}  XB: {X@beta}\")\n",
    "            \n",
    "dcd(X, Y, beta0, alpha0)                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
