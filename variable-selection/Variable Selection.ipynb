{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection\n",
    "\n",
    "We want to implement forward selection to get the list of best parameters out of six possibles. The possible parameters are the coefficients of the following linear regression model:\n",
    "\n",
    "$\\hat{y}={\\theta_0}+{\\theta_1}x_1+{\\theta_2}x_2+{\\theta_3}x_1^2+{\\theta_4}x_1x_2+{\\theta_5}x_2^2$\n",
    "\n",
    "The parameter list are represented as a python list, with integer numbers referring to each parameter number. That is: [1,3] represent the list of parameters ${\\theta_1}$ and ${\\theta_3}$.\n",
    "\n",
    "As follows, we implement forward search in python and run the algorithm five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f =  open(\"tutorial4_2.txt\", \"r\")\n",
    "\n",
    "#Reading lines: help taken from PythonForBeginners.com\n",
    "lines = f.readlines()\n",
    "\n",
    "#Processing each line to separate the pair of numbers\n",
    "lines1 = [l[:-2].split(\" \") for l in lines ]\n",
    "\n",
    "#converting to array to better processing\n",
    "data = np.array(lines1)\n",
    "\n",
    "#converting from string to float\n",
    "data = data.astype(float)\n",
    "\n",
    "#Separating data\n",
    "x1 = data[:,0]\n",
    "x2 = data[:,1]\n",
    "y = data[:,2]\n",
    "n_samples = data.shape[0]\n",
    "\n",
    "X = np.vstack(( np.ones((n_samples)),x1, x2, x1**2, np.multiply(x1,x2), x2**2)).T\n",
    "\n",
    "train_pct=0.5\n",
    "\n",
    "def split_train_test(X, y, train_pct):\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    #shuffling indexes to separate train and test randoming\n",
    "    idx = np.arange(0,n_samples)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    #creating test indexes\n",
    "    train_idx = idx[:train_size]\n",
    "\n",
    "    #creating test indexes\n",
    "    test_idx = idx[train_size:]\n",
    "\n",
    "    X_train = X[train_idx,]\n",
    "    X_test = X[test_idx,]\n",
    "\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def error (X, beta, y):\n",
    "    \n",
    "    '''Computes the loss of linear regression (MSE).\n",
    "   The parameters are:\n",
    "    - X is the matrix of features\n",
    "    - beta is the vector of parameters for the linear regression\n",
    "    - y is the target vector     '''\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    y_pred = X*beta\n",
    "    out = np.mean(np.array(y_pred-y)**2)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def train_lr(X,y):\n",
    "    \n",
    "    '''Computes the parameter fitting for the linear regression \n",
    "    using closed form'''\n",
    "    \n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y).T\n",
    "    beta = np.linalg.inv(np.matrix(X).T*X)*X.T*y\n",
    "    \n",
    "    return beta\n",
    "\n",
    "def forward_search(X,y):\n",
    "    \n",
    "    train_pct= 0.5\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y, train_pct)\n",
    "    \n",
    "    parameters_selected=[] ###list of parameters selected: empty at the beginning\n",
    "    parameters = [0, 1,2,3,4,5] ### list of possible parameters (each one refers to a 'theta'\n",
    "    e_allbest = 10000 #initial error\n",
    "    v_best = 1 #initialition of the best\n",
    "    parameters_tmp = [] #temporal list useful for inner loop\n",
    "\n",
    "    while v_best !=-1:#stop when no parameter on the list improves performance\n",
    "\n",
    "        v_best = -1\n",
    "        e_best = e_allbest\n",
    "        print(\"current selected\",parameters_selected)\n",
    "\n",
    "        for v in parameters:\n",
    "\n",
    "            #add a new parameter to a temporary list of parameters\n",
    "            parameters_tmp = parameters_selected + [v]\n",
    "\n",
    "            #subsetting with the temporary list of parameters\n",
    "            X_train_sub = X_train[:,parameters_tmp]\n",
    "            X_test_sub = X_test[:, parameters_tmp]\n",
    "\n",
    "            #fitting beta with linear regression\n",
    "            beta = train_lr(X_train_sub, y_train)\n",
    "\n",
    "            #test the fitted model with the new set of parameters\n",
    "            e = error(X_test_sub, beta, y_test)\n",
    "\n",
    "            if(e<e_best):      \n",
    "                v_best = v\n",
    "                e_best = e\n",
    "\n",
    "        ###if the new added parameter improves performance, then hold it\n",
    "        if (e_best< e_allbest):\n",
    "            parameters_selected.append(v_best)\n",
    "            idx = parameters.index(v_best)\n",
    "            parameters.pop(idx)\n",
    "            e_allbest=e_best\n",
    "    \n",
    "    print(\"Final list of parameters:\", parameters_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1:\n",
      "current selected []\n",
      "current selected [5]\n",
      "current selected [5, 3]\n",
      "Final list of parameters: [5, 3]\n",
      "Run 2:\n",
      "current selected []\n",
      "current selected [5]\n",
      "current selected [5, 3]\n",
      "current selected [5, 3, 4]\n",
      "current selected [5, 3, 4, 2]\n",
      "Final list of parameters: [5, 3, 4, 2]\n",
      "Run 3:\n",
      "current selected []\n",
      "current selected [3]\n",
      "current selected [3, 5]\n",
      "current selected [3, 5, 4]\n",
      "current selected [3, 5, 4, 2]\n",
      "current selected [3, 5, 4, 2, 1]\n",
      "Final list of parameters: [3, 5, 4, 2, 1]\n",
      "Run 4:\n",
      "current selected []\n",
      "current selected [5]\n",
      "current selected [5, 3]\n",
      "Final list of parameters: [5, 3]\n",
      "Run 5:\n",
      "current selected []\n",
      "current selected [5]\n",
      "current selected [5, 3]\n",
      "current selected [5, 3, 4]\n",
      "Final list of parameters: [5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Run 1:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 2:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 3:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 4:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 5:\")\n",
    "forward_search(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results are not always the same since there are some randomnes when splitting the dataset. The first two parameters added to the model are [3,5], that is, $\\theta_3$ and $\\theta_5$,since they represent the ground truth ($y=x_1^2-x_2^2$). \n",
    "\n",
    "To make improvements to the model and select only the true variables, we can set a threshold to the difference of the previous error and the current error, so that we avoid to add a new variable to the parameter set if the improvement in the error is too low. If the difference is too low (less than the threshold), we stop adding variables. As we show in the following run of the modified function, we can see that a threshold of 1 to the difference of errors (last_error - current_error), would give the desired subset of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1:\n",
      "current selected []\n",
      "Difference of errors with added variable: 9987.19655222\n",
      "current selected [5]\n",
      "Difference of errors with added variable: 11.8894959469\n",
      "current selected [5, 3]\n",
      "Difference of errors with added variable: 0.00490031564319\n",
      "current selected [5, 3, 4]\n",
      "Final list of parameters: [5, 3, 4]\n",
      "Run 2:\n",
      "current selected []\n",
      "Difference of errors with added variable: 9987.50941561\n",
      "current selected [5]\n",
      "Difference of errors with added variable: 11.6022202511\n",
      "current selected [5, 3]\n",
      "Difference of errors with added variable: 0.000961204328476\n",
      "current selected [5, 3, 2]\n",
      "Final list of parameters: [5, 3, 2]\n",
      "Run 3:\n",
      "current selected []\n",
      "Difference of errors with added variable: 9987.67488341\n",
      "current selected [5]\n",
      "Difference of errors with added variable: 11.3577375064\n",
      "current selected [5, 3]\n",
      "Difference of errors with added variable: 0.000970433684546\n",
      "current selected [5, 3, 2]\n",
      "Final list of parameters: [5, 3, 2]\n",
      "Run 4:\n",
      "current selected []\n",
      "Difference of errors with added variable: 9988.15582196\n",
      "current selected [3]\n",
      "Difference of errors with added variable: 10.9688193512\n",
      "current selected [3, 5]\n",
      "Final list of parameters: [3, 5]\n",
      "Run 5:\n",
      "current selected []\n",
      "Difference of errors with added variable: 9987.20434462\n",
      "current selected [5]\n",
      "Difference of errors with added variable: 11.7893094799\n",
      "current selected [5, 3]\n",
      "Difference of errors with added variable: 0.00335311900379\n",
      "current selected [5, 3, 4]\n",
      "Difference of errors with added variable: 0.000171848791325\n",
      "current selected [5, 3, 4, 2]\n",
      "Final list of parameters: [5, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_search(X,y):\n",
    "    \n",
    "    train_pct= 0.5\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y, train_pct)\n",
    "    \n",
    "    parameters_selected=[] ###list of parameters selected: empty at the beginning\n",
    "    parameters = [0, 1,2,3,4,5] ### list of possible parameters (each one refers to a 'theta'\n",
    "    e_allbest = 10000 #initial error\n",
    "    v_best = 1 #initialition of the best\n",
    "    parameters_tmp = [] #temporal list useful for inner loop\n",
    "\n",
    "    while v_best !=-1:#stop when no parameter on the list improves performance\n",
    "\n",
    "        v_best = -1\n",
    "        e_best = e_allbest\n",
    "        print(\"current selected\",parameters_selected)\n",
    "\n",
    "        for v in parameters:\n",
    "\n",
    "            #add a new parameter to a temporary list of parameters\n",
    "            parameters_tmp = parameters_selected + [v]\n",
    "\n",
    "            #subsetting with the temporary list of parameters\n",
    "            X_train_sub = X_train[:,parameters_tmp]\n",
    "            X_test_sub = X_test[:, parameters_tmp]\n",
    "\n",
    "            #fitting beta with linear regression\n",
    "            beta = train_lr(X_train_sub, y_train)\n",
    "\n",
    "            #test the fitted model with the new set of parameters\n",
    "            e = error(X_test_sub, beta, y_test)\n",
    "\n",
    "            if(e<e_best):      \n",
    "                v_best = v\n",
    "                e_best = e\n",
    "\n",
    "        ###if the new added parameter improves performance, then hold it\n",
    "        if (e_best< e_allbest):\n",
    "            parameters_selected.append(v_best)\n",
    "            idx = parameters.index(v_best)\n",
    "            parameters.pop(idx)\n",
    "            \n",
    "            #calculating difference between previous and current error\n",
    "            print(\"Difference of errors with added variable:\", e_allbest-e_best)\n",
    "            e_allbest=e_best\n",
    "            \n",
    "    \n",
    "    print(\"Final list of parameters:\", parameters_selected)\n",
    "\n",
    "print(\"Run 1:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 2:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 3:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 4:\")\n",
    "forward_search(X, y)\n",
    "print(\"Run 5:\")\n",
    "forward_search(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
