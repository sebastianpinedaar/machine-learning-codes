{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks\n",
    "\n",
    "In this notebook, we aim to implement and explain the algorithm for multi-dimensional series classification proposed by Yi Zheng et. al \"Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks\" [1]. The notebook comprises the following sections:\n",
    "\n",
    "* Explanation of the architecture\n",
    "* Explanation of the back-propagation\n",
    "* Implementation of the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture design can be summarized in the following main points:\n",
    "\n",
    "- The input signal may have multiple channel (multiple time series). They depict an image for a signal with three time series.\n",
    "- There are three main layer stages: 2 convolutional feature extractyor and a fully connected neural network.\n",
    "- The first convolutional layer comprises: 8 filter extrators of size 5, a subsampling layer (max-pooling) of size 2 and a sigmoid activation function.\n",
    "- The second convolutional layer compriceses: 4 filter extractor of size 5, a subsampling layer and a sigoid activation layer.\n",
    "- The final fully onnected layer comprises a hidden connecter layer with 732 neurons and output layer with 4 output neurons.\n",
    "\n",
    "The previous stages are represented graphically in the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/architecture](images/architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Explanation of the back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The back-propagation look for minimize the cost function, which for the paper is the cross entropy function (associated to four classes). To minimize this function, we find the derivative of the cost function respect to the parameters of the network (gradient of the paraemters) and then update the parameters according to gradient descent algorithm (plus using momentum and weight decay). Therefore we can summarize the gradient-based learning (as explained in section 3.2.) in the following way:\n",
    "\n",
    "- Feedforward pass: computation of the outputs of all the layers given the training data\n",
    "- Backpropagation pass: computing the gradient using the chain rule\n",
    "- Gradients applied: update of the parameters based on the gradients and using momentum and weight decay\n",
    "\n",
    "The parameters of the netowrk are the kernel values and the weights of the fully connected neural network. In total, there are: 40 parameters for the first stage, 20 parameters for the second stage and 2928 parameters for the hidden layer. The updating of the weights of the neural netowrk are a standard procedure, already exhaustively explained in the  literature. However, the auhtos show explicitely the derivation of the gradient for the kernel parameters. Applying the chain rule over the whole network, it is possible also then to find the gradient of the kernel parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the paper\n",
    "\n",
    "We want to implement the paper in Tensorflow, however, first we mut do a preprocessing of the raw data so that we can create the learning algorithm. \n",
    "\n",
    "The steps to carry out the implementation are: \n",
    "\n",
    "1. Load the data and select the activity and time series of interest (suggested in the paper):\n",
    "\n",
    "    - Standing\n",
    "    - Walking\n",
    "    - Ascending stairs\n",
    "    - Descending staris\n",
    "\n",
    "    Focus only on the 3D signal of the IMU hand acceleromenter in this notebook. In the paper, however, it is not clear which signal they use in the paper. Furthermore, we also delete those row with at least one missing value.\n",
    "\n",
    "3. Create the subsequences through a sliding window which stores subsets of the data (256 timestamps) as a single sample for training. The step-size for the sliding window is fixed to 32, while in the paper they try several. The smaller the step size, the more data we have to train. The sliding window size (=256) is the same as in the paper.\n",
    "\n",
    "4. Standarize every dimension of the sliding windows from the training and test set.\n",
    "\n",
    "5. Create the graph implementing the architecture shown in the first section.\n",
    "\n",
    "6. Suscribe the plots to tensorboard to visualize results. To make the tensorboard plots, we base on code from [4].\n",
    "\n",
    "7. Train the network with mini-batch gradiend descent. To create the minibatches, we use the useful code from [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, selecting and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import math \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#reading data\n",
    "path = \"PAMAP2_Dataset\\Protocol\"\n",
    "data_files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: subject102.dat  got  (90664, 3)\n",
      "File name: subject103.dat  got  (75233, 3)\n",
      "File name: subject104.dat  got  (87617, 3)\n",
      "File name: subject105.dat  got  (81173, 3)\n",
      "File name: subject106.dat  got  (74640, 3)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_file(file_name):\n",
    "    \n",
    "    \"\"\"This function preprocesses the subject file which is given as input\"\"\"\n",
    "    \n",
    "    #reasing the data\n",
    "    data_path = os.path.join(path, file_name)\n",
    "    file = open(data_path, 'r')\n",
    "    text = file.read()\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines_data = [l.split(\" \") for l in lines]\n",
    "    data = np.array(lines_data[:-1]) #discarding last element because it is free\n",
    "    data[data==\"NaN\"]= np.nan #renaming NaN\n",
    "    data_tf = data.astype(float)\n",
    "\n",
    "    #Activities ID of interest:\n",
    "    # - Standing (3)\n",
    "    # - Walking (4)\n",
    "    # - Ascending stairs (12)\n",
    "    # - Descending staits (13)\n",
    "\n",
    "    #Filtering activities\n",
    "    data_fil = np.vstack((data_tf[data_tf[:,1] == 3,],\n",
    "                          data_tf[data_tf[:,1] == 4,],\n",
    "                          data_tf[data_tf[:,1] == 12,],\n",
    "                          data_tf[data_tf[:,1] == 13,]))\n",
    "    \n",
    "    #filtering only sensor measurements of interest: subject108 and subject109 \n",
    "    #are left apart since they don't have data usful for the current application\n",
    "    X = data_fil[:, 4:7] #3D-acceleartion data from IMU hand\n",
    "    Y = data_fil[:,1 ].astype(int).reshape(-1,1) #label: activity ID\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#selecting files of interest - splitting in train and test\n",
    "data_files_train = data_files[:6]\n",
    "data_files_test = data_files[7]\n",
    "\n",
    "\n",
    "#preprocessing all the files and merging in a single file\n",
    "X_train, Y_train = preprocess_file(data_files_train[0])\n",
    "\n",
    "for file in data_files_train[1:]:\n",
    "    \n",
    "    temp_X, temp_Y = preprocess_file(file)\n",
    "    print(\"File name:\", file, \" got \", temp_X.shape)\n",
    "    X_train = np.vstack((X_train, temp_X))\n",
    "    Y_train = np.vstack((Y_train, temp_Y))\n",
    "\n",
    "#preprocessing the data for test set\n",
    "X_test, Y_test = preprocess_file(data_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before...\n",
      "Train: (484086, 3)\n",
      "Test: (78031, 3)\n",
      "Shape before...\n",
      "Train: (479208, 3)\n",
      "Test: (77521, 3)\n"
     ]
    }
   ],
   "source": [
    "#selecting the rows f the data with at least one NA\n",
    "select_train = np.isnan(X_train).any(axis=1)\n",
    "select_test = np.isnan(X_test).any(axis=1)\n",
    "\n",
    "print(\"Shape before...\")\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Test:\", X_test.shape)\n",
    "\n",
    "#deleting rows with at least one NA in train and test set\n",
    "X_train = X_train[ ~select_train, :]\n",
    "Y_train = Y_train[ ~select_train, :]\n",
    "X_test = X_test[ ~select_test, :]\n",
    "Y_test = Y_test[ ~select_test, :]\n",
    "\n",
    "print(\"Shape before...\")\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14974, 256, 3)\n",
      "(2421, 256, 3)\n",
      "Train size: 14784\n",
      "Test size: 2391\n",
      "Num. classes: 4\n"
     ]
    }
   ],
   "source": [
    "#contructing subsequences (trianing samples)\n",
    "step = 32\n",
    "size_subseq = 256\n",
    "\n",
    "def construct_tensor(X, Y, step, size_subseq):\n",
    "\n",
    "    \"\"\"Construct the subsequences of data. At the end, every subsequence is\n",
    "    a tensor of shape (number of steps, size_subseq, 3).  The number of steps,\n",
    "    therefore, is the number training samples and the 3 means that there are \n",
    "    three channels(because it is a 3D signal).\"\"\"\n",
    "    \n",
    "    size_t = X.shape[0]\n",
    "    X_pre = np.zeros(((size_t//step)-1, size_subseq,3))\n",
    "    Y_pre = np.zeros((size_t//step-1))\n",
    "    \n",
    "    print(X_pre.shape)\n",
    "    for i, j in enumerate(range(0,size_t, step)):\n",
    "        y_i = Y[j]\n",
    "        y_f = Y[min(j+size_subseq-1, size_t-1)]\n",
    "        temp = X[j:(j+size_subseq),]\n",
    "        if(y_i==y_f and temp.shape[0]==size_subseq):\n",
    "            X_pre[i,:,:]= temp\n",
    "            Y_pre[i] = Y[j]\n",
    "    Y_pre = np.array(Y_pre)\n",
    "    X_pre = np.delete(X_pre, np.where(Y_pre==0), 0)\n",
    "    Y_pre = np.delete(Y_pre, np.where(Y_pre==0))\n",
    "            \n",
    "    return X_pre, np.array(pd.get_dummies(Y_pre))\n",
    "\n",
    "#constructing sequences for train and test set\n",
    "X_train_pre, Y_train_pre = construct_tensor(X_train, Y_train, step, size_subseq)\n",
    "X_test_pre, Y_test_pre = construct_tensor(X_test, Y_test, step, size_subseq)\n",
    "\n",
    "train_size = X_train_pre.shape[0]\n",
    "test_size = X_test_pre.shape[0]\n",
    "n_classes = Y_train_pre.shape[1]\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Test size:\", test_size)\n",
    "print(\"Num. classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth = X_train_pre.shape[2]\n",
    "\n",
    "#creating the objects to scale\n",
    "scalers = [StandardScaler() for i in range(depth)]\n",
    "\n",
    "x_train = np.zeros(X_train_pre.shape)\n",
    "x_test = np.zeros(X_test_pre.shape)\n",
    "\n",
    "for i in range(depth):\n",
    "    x_train[:,:,i] = scalers[i].fit_transform(X_train_pre[:,:,i])\n",
    "    x_test[:,:,i] = scalers[i].transform(X_test_pre[:,:,i])\n",
    "\n",
    "y_train = Y_train_pre\n",
    "y_test = Y_test_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining functions\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\" \n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:]\n",
    "    shuffled_Y = Y[ permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[ k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-6c3ab86c6927>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iteration  0\n",
      "cost train: 1.3394886\n",
      "acc train: 0.67376894\n",
      "cost test: 2.3113377\n",
      "acc test: 0.49853617\n",
      "Iteration  20\n",
      "cost train: 0.5494436\n",
      "acc train: 0.80654764\n",
      "cost test: 1.6182616\n",
      "acc test: 0.58678377\n",
      "Iteration  40\n",
      "cost train: 0.51511633\n",
      "acc train: 0.8321834\n",
      "cost test: 0.86081266\n",
      "acc test: 0.84190714\n",
      "Iteration  60\n",
      "cost train: 0.530075\n",
      "acc train: 0.83630955\n",
      "cost test: 0.9031895\n",
      "acc test: 0.8318695\n",
      "Iteration  80\n",
      "cost train: 0.52028954\n",
      "acc train: 0.8475379\n",
      "cost test: 1.1499143\n",
      "acc test: 0.8209954\n"
     ]
    }
   ],
   "source": [
    "#initializing hyperparameters\n",
    "n_iterations = 100\n",
    "lr = 0.01\n",
    "batch_size = 64\n",
    "momentum= 0.9\n",
    "reg_weight = 0.00025\n",
    "\n",
    "#initializing graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#creating placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, size_subseq, depth))\n",
    "y = tf.placeholder(tf.float32, shape=(None, n_classes))\n",
    "\n",
    "#creating variables for the convolutional layers of three channels\n",
    "conv1_set = [tf.Variable(tf.truncated_normal(shape=[5, 1, 8], mean=0, stddev=0.1)) for i in range(depth)]\n",
    "conv2_set = [tf.Variable(tf.truncated_normal(shape=[5, 8, 4], mean=0, stddev=0.1)) for i in range(depth)]\n",
    "\n",
    "#creating variables for the fully connected\n",
    "W = tf.Variable(tf.truncated_normal(shape=[depth*4*61, 732], mean=0, stddev=0.1))\n",
    "bias = tf.Variable(tf.truncated_normal(shape=[732], mean=0, stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal(shape=[732, n_classes], mean=0, stddev=0.1))\n",
    "bias2 = tf.Variable(tf.truncated_normal(shape=[n_classes], mean=0, stddev=0.1))\n",
    "\n",
    "\n",
    "#creating graph for the first convolutional layer\n",
    "conv11 = [tf.nn.conv1d(tf.reshape(x[:,:,i], (-1, size_subseq, 1)), \n",
    "                       conv1_set[i], stride=1, padding='VALID') for i in range(depth)]\n",
    "pool11 = [tf.layers.max_pooling1d(conv11[i], pool_size=2, strides=2, padding='valid') for i in range(depth)]\n",
    "activation11 = [tf.nn.sigmoid(pool11[i]) for i in range(depth)]\n",
    "\n",
    "#creating graph for the second convolutional layer\n",
    "conv22 = [tf.nn.conv1d(activation11[i], conv2_set[i], stride=1, padding='VALID') for i in range(depth)]\n",
    "pool22 = [tf.layers.max_pooling1d(conv22[i], pool_size=2, strides=2, padding='valid') for i in range(depth)]\n",
    "activation22 = [tf.nn.sigmoid(pool22[i]) for i in range(depth)]\n",
    "\n",
    "#creating graph for the fully connected layer\n",
    "flat1 = [tf.contrib.layers.flatten(activation22[i]) for i in range(depth)]\n",
    "stack = tf.concat(flat1, axis=1)\n",
    "h = tf.matmul(stack, W)+bias #first fully connected layer\n",
    "conv_net_output = tf.matmul(h, W2)+bias2 #second fully connected layer\n",
    "\n",
    "#creating graph for regularizer\n",
    "regularizer = tf.nn.l2_loss(W) + tf.nn.l2_loss(W2) \n",
    "\n",
    "for i in range(depth):\n",
    "    regularizer += tf.nn.l2_loss(conv1_set[i])\n",
    "    regularizer += tf.nn.l2_loss(conv2_set[i])\n",
    "\n",
    "#loss and train step\n",
    "cost =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( labels=y, logits=conv_net_output)) + regularizer*reg_weight\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "#computing accuracy\n",
    "correct_pred = tf.equal(tf.argmax(conv_net_output, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "#obtaining gradients for debug in tensorboard\n",
    "grads = tf.train.AdamOptimizer(lr).compute_gradients(loss=cost)\n",
    "\n",
    "#suscribing tensors to plot in tensorboard\n",
    "for i in range(depth):\n",
    "    with tf.name_scope('conv1'):\n",
    "        variable_summaries(conv1_set[i])\n",
    "\n",
    "    with tf.name_scope('activation1'):\n",
    "        variable_summaries(activation11[i])\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        variable_summaries(conv2_set[i])\n",
    "\n",
    "    with tf.name_scope('activation2'):\n",
    "        variable_summaries(activation22[i])\n",
    "\n",
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    variable_summaries(conv_net_output)\n",
    "\n",
    "for grad in grads:\n",
    "    with tf.name_scope('grads'):\n",
    "        variable_summaries(grad)\n",
    "    \n",
    "#initializin graph and tensorboard writers\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "summ_writer_train = tf.summary.FileWriter(os.path.join('summaries','train'), sess.graph)\n",
    "summ_writer_test = tf.summary.FileWriter(os.path.join('summaries','test'), sess.graph)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "#minibatch gradient descent\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    minibatches = random_mini_batches(x_train, y_train, batch_size, 1)\n",
    "    \n",
    "    for j, minibatch in enumerate(minibatches):\n",
    "            idx = np.random.randint(0,train_size)\n",
    "            batch_X, batch_Y = minibatch\n",
    "            summ, _, cost_, accuracy_ = sess.run([merged, train_step, cost, accuracy],\n",
    "                                  feed_dict = {x:batch_X, y:batch_Y})\n",
    "              \n",
    "\n",
    "    summ_train, cost_train, accuracy_train = sess.run([ merged, cost, accuracy],\n",
    "                                  feed_dict = {x:x_train, y:y_train})\n",
    "    \n",
    "    summ_test, cost_test, accuracy_test = sess.run([ merged, cost, accuracy],\n",
    "                                  feed_dict = {x:x_test, y:y_test})\n",
    "    \n",
    "    if (i%20 == 0):\n",
    "        print(\"Iteration \",i)\n",
    "        print(\"cost train:\", cost_train)\n",
    "        print(\"acc train:\", accuracy_train)    \n",
    "        print(\"cost test:\", cost_test)\n",
    "        print(\"acc test:\", accuracy_test)\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the TensorBoard\n",
    "    summ_writer_train.add_summary(summ_train, i)\n",
    "    summ_writer_test.add_summary(summ_test, i)\n",
    "    summ_writer_train.flush()\n",
    "    summ_writer_test.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: (14784, 252, 8)\n",
      "conv2: (14784, 122, 4)\n",
      "activation1: (14784, 126, 8)\n",
      "activation2: (14784, 61, 4)\n",
      "flat: (14784, 244)\n",
      "concatenated: (14784, 732)\n",
      "full: (14784, 4)\n"
     ]
    }
   ],
   "source": [
    "conv1_, activation1_, conv2_, activation2_,  full_, flat_, stack_= sess.run([conv11[0], activation11[0], \n",
    "                conv22[0], activation22[0], conv_net_output, flat1[0], stack], feed_dict = {x:x_train, y:y_train})\n",
    "print(\"conv1:\", conv1_.shape) #output of the first convolution (one channel)\n",
    "print(\"conv2:\", conv2_.shape) #output of the second convolution (one channel)\n",
    "print(\"activation1:\", activation1_.shape) #size of the first activation (one channel)\n",
    "print(\"activation2:\", activation2_.shape) #size of the first activation (one channel)\n",
    "print(\"flat:\", flat_.shape) #size after flatten (one channel)\n",
    "print(\"concatenated:\", stack_.shape) #size after concatenating\n",
    "print(\"full:\", full_.shape) #size of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots in tensorboard\n",
    "\n",
    "In the following picture, we show some of the plots made in TensorBoard:\n",
    "\n",
    "- Cost and accuracy (blue= train, red=test)\n",
    "<img src=\"images/cost_accuracy2.jpg\">\n",
    "\n",
    "- Statisics of activations in the fist stage (different channels)\n",
    "<img src=\"images/statistics_activation_1.jpg\">\n",
    "\n",
    "- Statisics of activations in the second stage (different channels)\n",
    "<img src=\"images/statistics_activation_2.jpg\">\n",
    "\n",
    "- Statisics of gradients in the fist stage (different channels)\n",
    "<img src=\"images/statistics_of_gradients_1st_layer.jpg\">\n",
    "\n",
    "- Distributions of some gradients\n",
    "<img src=\"images/distributions_of_some_gradients.jpg\">\n",
    "\n",
    "- Histograms of first convolutional filters\n",
    "<img src=\"images/histograms_of_first_convolutional_filters.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and comments about implementation\n",
    "\n",
    "In the paper there are some points that are not clear, and therefore we make some assumptions that may not meet the original ideas.\n",
    "\n",
    "- The authors don't specify which 3D times series they use (which columns from the data file). There are several possibles subsets from the original dataset [5]. For simplicity, and speed in the preprocessing, we onyl work with the accelerometer signals from the hand (IMU hand).\n",
    "- The implementation of the gradient update referenced in the paper could be done with *tf.train.MomentumOptimizer(learning_rate=lr, momentum= momentum, use_nesterov=True)*. However, we achieved better results with *AdamOptimizer* (which is the state of the art optimizer and widley used). To emualte the weight decay (=0.0005), we use L2-regularization (=0.00025).\n",
    "- In the paper they specify that they use SGD. However, we use mini-batch (batch size= 64), since we were able to see softer convergence plots.\n",
    "- We hiphotetize that because of the unexact specification of the columns to use, we are not able to reproduce completely the results. In our case, as we can see in above plots, we achieved an accuracy of 85.03% in test while the paper report an accuracy of 91.14% using step=32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References:\n",
    "\n",
    "[1] Zheng Y., Liu Q., Chen E., Ge Y., Zhao J.L. (2014) *Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks.* In: Li F., Li G., Hwang S., Yao B., Zhang Z. (eds) Web-Age Information Management. WAIM 2014. Lecture Notes in Computer Science, vol 8485. Springer, Cham\n",
    "\n",
    "[2] Code for mini-batches taken from: https://github.com/andersy005/deep-learning-specialization-coursera/blob/master/02-Improving-Deep-Neural-Networks/week3/Programming-Assignments/tf_utils.py\n",
    "\n",
    "[3] Regularization: https://markojerkic.com/build-a-multi-layer-neural-network-with-l2-regularization-with-tensorflow/\n",
    "\n",
    "[4] Tensorboard: https://www.tensorflow.org/guide/summaries_and_tensorboard\n",
    "\n",
    "[5] PAMP2 Dataset: http://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
