{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1><CENTER> Machine Learning Lab - Exercise Sheet 9 <br /></CENTER></H1>\n",
    "<H2><CENTER> Author: Sebastian Pineda Arango  <br />\n",
    "ID: 246098<br />\n",
    "Universität Hildesheim - Data Analytics Master </CENTER></H2>\n",
    "\n",
    "<img src=\"https://www.uni-hildesheim.de/typo3conf/ext/unihildesheim/Resources/Public/Images/logo-uni.svgz\" width=\"100\">\n",
    "\n",
    "In this notebook, we want to develop spam classifier using suppoert vector machines. For that, we use two libraries: LIBSVM and Scikit-learn.\n",
    "\n",
    "The library LIBSVM was installed from this repository: https://www.lfd.uci.edu/~gohlke/pythonlibs/#libsvm\n",
    "\n",
    "Where we use the following command for the installation:\n",
    "\n",
    "Command used: \"pip install libsvm-3.23-cp36-cp36m-win_amd64.whl\"\n",
    "\n",
    "Some help to change the parameters of the libsvm model was takne from:\n",
    "https://lmb.informatik.uni-freiburg.de/lectures/old_lmb/svm_seminar/java/libsvmdemo.html\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Two datasets are used to create the spam classifier. The datasets are donwloaded from the following sources: \n",
    "* Dataset 1: \n",
    "https://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "* Dataset 2:\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "\n",
    "#### Support vector machines\n",
    "\n",
    "The support vecotr machines is, in general, a linear classifeir which aims to find a separating hyperplane so that the margin between one class and another is maximized. The idea of margin is depicted in the following image (own authorship):\n",
    "\n",
    "<img src=\"images/svm.jpg\" width=\"300\">\n",
    "\n",
    "That means that the support vector machines optimization problem could be expressed in the following way [2]:\n",
    "\n",
    "$\\min_{\\textbf{w},b}    \\frac{1}2||\\textbf{w}||$\n",
    "\n",
    "$s.t. \\, y_i(\\textbf{w}^T\\textbf{x}_i+b)\\geq 1$\n",
    "\n",
    "## Exercise 1: Spam filter using SVM\n",
    "\n",
    "A spam filter is created using spam dataset 1 and the library \"libsvm\". This library accept format in the following way (libsvm format):\n",
    "\n",
    "$<label> \\space <index1>:<value> \\space <index2>:<value2> \\space...$\n",
    "\n",
    "There is a function in scikit-learn that enables the creation of dataset in this format: $load\\_svm\\_light\\_file ()$. However, we implement the function to convert a sparse matriz into this libsvm format. The following steps are performed:\n",
    "\n",
    "* Importing libraries\n",
    "* Importing data\n",
    "* Exploring data\n",
    "* Splitting data in train-test\n",
    "* Cross validation\n",
    "* Choosing the best final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from svmutil import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing data\n",
    "data1 = pd.read_csv(\"data\\spambase\\spambase.data\", header=None)\n",
    "data1.rename(columns={57:\"y\"}, inplace = 0.6)\n",
    "\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9 ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  y  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278  1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring the data\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    2788\n",
       "1    1813\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.groupby('y')['y'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y value: 0.39404477287546186\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of y value:\", np.mean(data3['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data, we have the following insights:\n",
    "\n",
    "* There are 4601 samples and 57 features. \n",
    "* The class label (y), which corresponds to 1 when it is spam and to 0 when it is not. There is 39.4% of data from class not-spam.\n",
    "* If we look the documentation, the features are divided in the following way:\n",
    "    - 48 features corresponds to the frequency of the word (in percentage). Where they take the frequency of the word, and divide it by the total number of words of the e-mail.\n",
    "    - 6 features are percentage of given chars on the email.\n",
    "    - 1 feature for the average length of uninterrupted sequence of capital letters.\n",
    "    - 1 feature for the average length of uninterrupted sequence of capital letters.\n",
    "    - 1 feature for the sum of length of uninterrupted sequences of capital letters.\n",
    "    \n",
    "Now we split data in training and test, where we assing 80% of the data to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting train and test\n",
    "\n",
    "def split_train_test(data, train_pct, features, target):\n",
    "    \n",
    "    '''This functions divides \"data\"  in train and test set.\n",
    "    The percentage give to the train data is determined by \"train_pct\".\n",
    "    The \"features\" argument determine a list of features to consider.\n",
    "    The \"target\" arugment indicates the the variable to predict.'''\n",
    "    \n",
    "    #getting the total number of training samples\n",
    "    data_size = data.shape[0]\n",
    "    train_size = int(train_pct*data_size)\n",
    "\n",
    "    #shuffling indexes to separate train and test randoming\n",
    "    idx = np.arange(0,data_size)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    #creating test indexes\n",
    "    train_idx = idx[:train_size]\n",
    "\n",
    "    #creating test indexes\n",
    "    test_idx = idx[train_size:]\n",
    "\n",
    "    #selecting train data (features)\n",
    "    X_train = data[features].iloc[train_idx,]\n",
    "\n",
    "    #selecting train data (target)\n",
    "    y_train = data[target].iloc[train_idx,]\n",
    "\n",
    "    #selecting test data (features)\n",
    "    X_test = data[features].iloc[test_idx,]\n",
    "\n",
    "    #selecting test data (target)\n",
    "    y_test = data[target].iloc[test_idx,]\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3680\n",
      "Test size: 921\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data3.iloc[:,:-1])\n",
    "y = np.array(data3.iloc[:,-1])\n",
    "col_names = data3.columns[:-1]\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_train_test(data1, 0.8, col_names, 'y')\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Train size:\",X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we store the train data in libsvm format, so that we can use it in future implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as sk_dt\n",
    "\n",
    "sk_dt.dump_svmlight_file(X_train, y_train, 'train_data')\n",
    "sk_dt.dump_svmlight_file(X_test, y_test, 'test_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that converts sparse matrices (with a lot of zero values) into _libsvm_ format. Having this, we can input the matrices to the _libsvm_ train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_format(X):\n",
    "\n",
    "    '''This function converts X in a libsvm format.'''\n",
    "    \n",
    "    n_train = X.shape[0]\n",
    "\n",
    "    #list to store the new format\n",
    "    X_dok_format = []\n",
    "    \n",
    "    #iterating over all the data\n",
    "    for i in range(n_train):\n",
    "        \n",
    "        #getting indexes different to zero\n",
    "        key=np.where(X[i,]!=0)\n",
    "        \n",
    "        #getting values of indexes different to zero\n",
    "        val=X[i,key]\n",
    "        \n",
    "        #creating a dictionary with the last values\n",
    "        x= dict(zip(list(key[0]),list(val[0])))\n",
    "        \n",
    "        #appending the dictionary (libsvm format) to list\n",
    "        X_dok_format.append(x)\n",
    "        \n",
    "    return X_dok_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective now is to pick the best hyperparameter C. To do that,we apply k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying c= 0.01\n",
      "Accuracy = 60.0204% (1767/2944) (classification)\n",
      "Accuracy = 62.0924% (457/736) (classification)\n",
      "Accuracy = 60.0204% (1767/2944) (classification)\n",
      "Accuracy = 60.1902% (443/736) (classification)\n",
      "Accuracy = 59.1372% (1741/2944) (classification)\n",
      "Accuracy = 61.9565% (456/736) (classification)\n",
      "Accuracy = 60.428% (1779/2944) (classification)\n",
      "Accuracy = 56.7935% (418/736) (classification)\n",
      "Accuracy = 59.7826% (1760/2944) (classification)\n",
      "Accuracy = 59.375% (437/736) (classification)\n",
      "Trying c= 0.1\n",
      "Accuracy = 87.7717% (2584/2944) (classification)\n",
      "Accuracy = 72.1467% (531/736) (classification)\n",
      "Accuracy = 87.7717% (2584/2944) (classification)\n",
      "Accuracy = 73.7772% (543/736) (classification)\n",
      "Accuracy = 87.2962% (2570/2944) (classification)\n",
      "Accuracy = 78.125% (575/736) (classification)\n",
      "Accuracy = 79.0421% (2327/2944) (classification)\n",
      "Accuracy = 63.8587% (470/736) (classification)\n",
      "Accuracy = 86.9565% (2560/2944) (classification)\n",
      "Accuracy = 75.1359% (553/736) (classification)\n",
      "Trying c= 1\n",
      "Accuracy = 95.9239% (2824/2944) (classification)\n",
      "Accuracy = 81.25% (598/736) (classification)\n",
      "Accuracy = 95.9239% (2824/2944) (classification)\n",
      "Accuracy = 80.8424% (595/736) (classification)\n",
      "Accuracy = 95.6861% (2817/2944) (classification)\n",
      "Accuracy = 80.2989% (591/736) (classification)\n",
      "Accuracy = 95.4823% (2811/2944) (classification)\n",
      "Accuracy = 82.0652% (604/736) (classification)\n",
      "Accuracy = 95.8899% (2823/2944) (classification)\n",
      "Accuracy = 82.4728% (607/736) (classification)\n",
      "Trying c= 2\n",
      "Accuracy = 97.4185% (2868/2944) (classification)\n",
      "Accuracy = 81.3859% (599/736) (classification)\n",
      "Accuracy = 97.4185% (2868/2944) (classification)\n",
      "Accuracy = 81.7935% (602/736) (classification)\n",
      "Accuracy = 97.5883% (2873/2944) (classification)\n",
      "Accuracy = 81.25% (598/736) (classification)\n",
      "Accuracy = 97.5883% (2873/2944) (classification)\n",
      "Accuracy = 82.7446% (609/736) (classification)\n",
      "Accuracy = 97.928% (2883/2944) (classification)\n",
      "Accuracy = 82.2011% (605/736) (classification)\n",
      "Trying c= 5\n",
      "Accuracy = 98.8111% (2909/2944) (classification)\n",
      "Accuracy = 82.2011% (605/736) (classification)\n",
      "Accuracy = 98.8111% (2909/2944) (classification)\n",
      "Accuracy = 82.337% (606/736) (classification)\n",
      "Accuracy = 99.1848% (2920/2944) (classification)\n",
      "Accuracy = 81.7935% (602/736) (classification)\n",
      "Accuracy = 98.913% (2912/2944) (classification)\n",
      "Accuracy = 83.1522% (612/736) (classification)\n",
      "Accuracy = 99.1168% (2918/2944) (classification)\n",
      "Accuracy = 82.337% (606/736) (classification)\n",
      "Trying c= 10\n",
      "Accuracy = 99.4905% (2929/2944) (classification)\n",
      "Accuracy = 81.3859% (599/736) (classification)\n",
      "Accuracy = 99.4905% (2929/2944) (classification)\n",
      "Accuracy = 81.25% (598/736) (classification)\n",
      "Accuracy = 99.5245% (2930/2944) (classification)\n",
      "Accuracy = 81.7935% (602/736) (classification)\n",
      "Accuracy = 99.4565% (2928/2944) (classification)\n",
      "Accuracy = 83.6957% (616/736) (classification)\n",
      "Accuracy = 99.4565% (2928/2944) (classification)\n",
      "Accuracy = 83.288% (613/736) (classification)\n",
      "Trying c= 20\n",
      "Accuracy = 99.7622% (2937/2944) (classification)\n",
      "Accuracy = 80.9783% (596/736) (classification)\n",
      "Accuracy = 99.7622% (2937/2944) (classification)\n",
      "Accuracy = 81.25% (598/736) (classification)\n",
      "Accuracy = 99.6264% (2933/2944) (classification)\n",
      "Accuracy = 81.9293% (603/736) (classification)\n",
      "Accuracy = 99.5924% (2932/2944) (classification)\n",
      "Accuracy = 84.7826% (624/736) (classification)\n",
      "Accuracy = 99.6943% (2935/2944) (classification)\n",
      "Accuracy = 83.4239% (614/736) (classification)\n",
      "Trying c= 50\n",
      "Accuracy = 99.8641% (2940/2944) (classification)\n",
      "Accuracy = 80.5707% (593/736) (classification)\n",
      "Accuracy = 99.8641% (2940/2944) (classification)\n",
      "Accuracy = 81.25% (598/736) (classification)\n",
      "Accuracy = 99.8302% (2939/2944) (classification)\n",
      "Accuracy = 80.9783% (596/736) (classification)\n",
      "Accuracy = 99.7283% (2936/2944) (classification)\n",
      "Accuracy = 83.9674% (618/736) (classification)\n",
      "Accuracy = 99.7962% (2938/2944) (classification)\n",
      "Accuracy = 83.0163% (611/736) (classification)\n",
      "Trying c= 100\n",
      "Accuracy = 99.9321% (2942/2944) (classification)\n",
      "Accuracy = 80.4348% (592/736) (classification)\n",
      "Accuracy = 99.9321% (2942/2944) (classification)\n",
      "Accuracy = 81.1141% (597/736) (classification)\n",
      "Accuracy = 99.9321% (2942/2944) (classification)\n",
      "Accuracy = 80.4348% (592/736) (classification)\n",
      "Accuracy = 99.8981% (2941/2944) (classification)\n",
      "Accuracy = 83.4239% (614/736) (classification)\n",
      "Accuracy = 99.8981% (2941/2944) (classification)\n",
      "Accuracy = 82.6087% (608/736) (classification)\n"
     ]
    }
   ],
   "source": [
    "#number of samples of training set\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "#number of folds\n",
    "n_folds = 5\n",
    "\n",
    "#initializing folds\n",
    "folds = []\n",
    "samples_fold = int(n_train/n_folds)\n",
    "\n",
    "#creating the k-fold subsets\n",
    "for i in range(n_folds):\n",
    "    \n",
    "    folds.append((X_train[(i*samples_fold):((i+1)*samples_fold),:],\n",
    "                   y_train[(i*samples_fold):((i+1)*samples_fold)]))\n",
    "\n",
    "folds_list = list(range(n_folds+1))\n",
    "\n",
    "#initialize list to store the man of each hyperparameter setting\n",
    "mean_test_folds = []\n",
    "mean_train_folds = []\n",
    "\n",
    "#list of hyperparameters\n",
    "c_list = [0.01,0.1,1,2,5,10,20,50,100]\n",
    "\n",
    "for c in c_list:\n",
    "\n",
    "    print(\"Trying c=\",c)\n",
    "    test_acc_folds = []\n",
    "    train_acc_folds = []\n",
    "    \n",
    "    for f in range(n_folds):\n",
    "\n",
    "            #list of folds\n",
    "            folds_list = list(range(n_folds))\n",
    "            folds_list.pop(f)\n",
    "\n",
    "            #selecting test dataset\n",
    "            X_test_fold = folds[f][0]\n",
    "            y_test_fold = folds[f][1]\n",
    "\n",
    "            #merging the folds to create the training dataset\n",
    "            X_train_fold = folds[folds_list[1]][0]   \n",
    "            y_train_fold = folds[folds_list[1]][1]\n",
    "            \n",
    "            for j in folds_list[1:]:\n",
    "                X_train_fold = np.vstack((X_train_fold, folds[j][0]))\n",
    "                y_train_fold = np.hstack((y_train_fold, folds[j][1]))\n",
    "            \n",
    "            #converting data to libsvm format\n",
    "            X_train_fold = convert_to_format(X_train_fold)\n",
    "            X_test_fold = convert_to_format(X_test_fold)\n",
    "            y_train_fold = list(y_train_fold)\n",
    "            y_test_fold = list(y_test_fold)\n",
    "            \n",
    "            #training the model\n",
    "            param = '-c '+ str(c)\n",
    "            \n",
    "            m = svm_train( y_train_fold, X_train_fold, param)\n",
    "            \n",
    "            acc_train=svm_predict(y_train_fold, X_train_fold, m)\n",
    "            acc_test = svm_predict(y_test_fold, X_test_fold, m)\n",
    "            \n",
    "            #finding accuracy over the fold\n",
    "            train_acc_folds.append(acc_train[1][0])\n",
    "            test_acc_folds.append(acc_test[1][0])\n",
    "            \n",
    "    #findning the mean across all the folds\n",
    "    mean_train_folds.append(np.mean(train_acc_folds))\n",
    "    mean_test_folds.append(np.mean(test_acc_folds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accucary vs. Hyperparameter C')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19c9772e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(c_list, mean_test_folds, 'b')\n",
    "plt.plot(c_list, mean_train_folds, 'r')\n",
    "plt.grid()\n",
    "plt.legend(['Test', 'Train' ])\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Accucary vs. Hyperparameter C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Average mean test</th>\n",
       "      <th>Average mean train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>60.081522</td>\n",
       "      <td>59.877717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>72.608696</td>\n",
       "      <td>85.767663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>81.385870</td>\n",
       "      <td>95.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.00</td>\n",
       "      <td>81.875000</td>\n",
       "      <td>97.588315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00</td>\n",
       "      <td>82.364130</td>\n",
       "      <td>98.967391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.00</td>\n",
       "      <td>82.282609</td>\n",
       "      <td>99.483696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.00</td>\n",
       "      <td>82.472826</td>\n",
       "      <td>99.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.00</td>\n",
       "      <td>81.956522</td>\n",
       "      <td>99.816576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100.00</td>\n",
       "      <td>81.603261</td>\n",
       "      <td>99.918478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  Average mean test  Average mean train\n",
       "0    0.01          60.081522           59.877717\n",
       "1    0.10          72.608696           85.767663\n",
       "2    1.00          81.385870           95.781250\n",
       "3    2.00          81.875000           97.588315\n",
       "4    5.00          82.364130           98.967391\n",
       "5   10.00          82.282609           99.483696\n",
       "6   20.00          82.472826           99.687500\n",
       "7   50.00          81.956522           99.816576\n",
       "8  100.00          81.603261           99.918478"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame({'C':c_list, 'Average mean train':mean_train_folds, 'Average mean test':mean_test_folds})\n",
    "\n",
    "df[[ 'C', 'Average mean test', 'Average mean train']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the table, the best model is the one with C=20, so we retrain the model with the whole train data set and compute accuracy on the test set. The accuracy on the test set is 85.342% which is close to the mean test accuracy of the k-fold cross validaiton (82.47%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 85.342% (786/921) (classification)\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_to_format(X_train)\n",
    "y_train = list(y_train)\n",
    "\n",
    "X_test = convert_to_format(X_test)\n",
    "y_test = list(y_test)\n",
    "\n",
    "m = svm_train(y_train, X_train, '-c 20')\n",
    "p=svm_predict(y_test, X_test, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part B: Pre-processing and learning SVM\n",
    "\n",
    "The seconde dataset is donwloaded This dataset is the mixture of different datasets of SMS message. They are labeled as SPAM or HAM. However, we get row text, therefore, the columns should be processed to get features that can be used on a learning algorithm. \n",
    "\n",
    "To perform this task, the following steps are done:\n",
    "\n",
    "* Importing the data\n",
    "* Importing stop_words\n",
    "* Converting to lower case\n",
    "* Taking out special characters and numbers\n",
    "* Counting word\n",
    "* Term frequency matrix\n",
    "* Grid seach\n",
    "* Choosing and training of final model\n",
    "* Evaluating final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing data\n",
    "data2 = pd.read_csv(\"data\\smsspamcollection\\SMSSpamCollection\", sep=\"\\t\", names=['Label', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing, the stop words are imported from the library \"stop_words\". This library contains a large list of stop words for english that can be used to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stop words: ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and']\n"
     ]
    }
   ],
   "source": [
    "#Importing stop words\n",
    "from stop_words import  get_stop_words\n",
    "import re\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "print(\"Some stop words:\", en_stop[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the text is processed so that all letters are converted to lower case, and numbers an special characters are eliminated. We also filter all stops words from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point crazy available bugis n great ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry  wkly comp win fa cup final tkts st...\n",
       "3                  u dun say early hor u c already say\n",
       "4          nah dont think goes usf lives around though\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.Text = data2.Text.apply(lambda x: x.lower()) ###All lower casetext\n",
    "data2.Text = data2.Text.apply(lambda x: re.sub(r'[^\\w\\s]','',x)) #only alpha-numeric and space\n",
    "data2.Text = data2.Text.apply(lambda x: re.sub(r'[0-9]','',x)) #filtering numbers\n",
    "\n",
    "#filtering stop words from all samples\n",
    "for stop_word in en_stop:\n",
    "    data2.Text = data2.Text.apply(lambda x: re.sub(r' '+stop_word+' ',' ', x) )\n",
    "\n",
    "data2.Text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (4457, 1)\n",
      "Shape of test: (1115, 1)\n"
     ]
    }
   ],
   "source": [
    "#splitting train and test\n",
    "X_train, y_train, X_test, y_test = split_train_test(data2, 0.8, ['Text'], 'Label')\n",
    "\n",
    "print(\"Shape of train:\", X_train.shape)\n",
    "print(\"Shape of test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert text to numbers, we perfomr a tf-idf transformation. TF-IDF transfrmation finds for each term $t$ in a document $d$ the normalized frequency ($tf$) and the inverse frequency ($idf$):\n",
    "\n",
    "$tf(t,d) = \\frac{f(t,d)}{max\\{f(t,d):t \\space \\epsilon \\space d\\}}$\n",
    "\n",
    "$idf(t,D) = log\\frac{|D|}{|\\{d \\space \\epsilon \\space D \\space : t \\space \\epsilon \\space d \\}|}$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $|D|$: is the number of samples (numer of documents)\n",
    "* $|\\{d \\space \\epsilon \\space D \\space : t \\space \\epsilon \\space d \\}|$: is the number of documents where the term $t$ appears\n",
    "\n",
    "Finally, the TFIDF is calculated as [1]:\n",
    "\n",
    "$tfidf(t,d,D) = tf(t,d)\\times idf(t,D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7597)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#getting the frequency of terms\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train.iloc[:,0])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x7597 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39296 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the tf-idf transformation\n",
    "tfidf_transf = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transf.transform(X_train_counts)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data\n",
    "X_test_counts = count_vect.transform(X_test.iloc[:,0])\n",
    "X_test_tfidf = tfidf_transf.transform(X_test_counts)\n",
    "X_test = X_test_tfidf.toarray()\n",
    "\n",
    "#transforming labels (originally the are strings)\n",
    "y_train = np.array([1 if i==\"ham\" else 0 for i in y_train])\n",
    "y_test = np.array([1 if i==\"ham\" else 0 for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best classifier:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "#creating the hyperaparameter list\n",
    "kernel_list = ['linear', 'rbf']\n",
    "c_list=[0.01, 0.1, 1, 10, 20, 30, 100]\n",
    "\n",
    "#creating initial classifier\n",
    "clf = svm.SVC( )\n",
    "\n",
    "#creating hyperparameter dicionary for grid searh\n",
    "parameters={'C':c_list, 'kernel':kernel_list}\n",
    "\n",
    "#creating scorer for grid search\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "#performing grid search\n",
    "svm_grid = GridSearchCV(clf, parameters, cv=5, scoring=f1_scorer)\n",
    "\n",
    "#fitting data\n",
    "svm_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#choosing the best estimator\n",
    "best_svm= svm_grid.best_estimator_\n",
    "\n",
    "print(\"Best classifier:\")\n",
    "best_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'C': 0.01, 'kernel': 'linear'},\n",
       " {'C': 0.01, 'kernel': 'rbf'},\n",
       " {'C': 0.1, 'kernel': 'linear'},\n",
       " {'C': 0.1, 'kernel': 'rbf'},\n",
       " {'C': 1, 'kernel': 'linear'},\n",
       " {'C': 1, 'kernel': 'rbf'},\n",
       " {'C': 10, 'kernel': 'linear'},\n",
       " {'C': 10, 'kernel': 'rbf'},\n",
       " {'C': 20, 'kernel': 'linear'},\n",
       " {'C': 20, 'kernel': 'rbf'},\n",
       " {'C': 30, 'kernel': 'linear'},\n",
       " {'C': 30, 'kernel': 'rbf'},\n",
       " {'C': 100, 'kernel': 'linear'},\n",
       " {'C': 100, 'kernel': 'rbf'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the hyperparameter set list\n",
    "svm_grid.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Mean F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.01, linear)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.01, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.1, linear)</td>\n",
       "      <td>0.953030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.1, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, linear)</td>\n",
       "      <td>0.986173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(10, linear)</td>\n",
       "      <td>0.986158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(10, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(20, linear)</td>\n",
       "      <td>0.986031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(20, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(30, linear)</td>\n",
       "      <td>0.986031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(30, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(100, linear)</td>\n",
       "      <td>0.986031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(100, rbf)</td>\n",
       "      <td>0.927704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 C   Mean F1\n",
       "0   (0.01, linear)  0.927704\n",
       "1      (0.01, rbf)  0.927704\n",
       "2    (0.1, linear)  0.953030\n",
       "3       (0.1, rbf)  0.927704\n",
       "4      (1, linear)  0.986173\n",
       "5         (1, rbf)  0.927704\n",
       "6     (10, linear)  0.986158\n",
       "7        (10, rbf)  0.927704\n",
       "8     (20, linear)  0.986031\n",
       "9        (20, rbf)  0.927704\n",
       "10    (30, linear)  0.986031\n",
       "11       (30, rbf)  0.927704\n",
       "12   (100, linear)  0.986031\n",
       "13      (100, rbf)  0.927704"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating table to plot\n",
    "x_axis = [(i['C'], i['kernel']) for i in svm_grid.cv_results_['params']]\n",
    "y_axis = svm_grid.cv_results_['mean_test_score']\n",
    "pd.DataFrame({'C':x_axis, 'Mean F1':y_axis })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882111737570477"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on new (test) data\n",
    "pred_test = best_svm.predict(X_test_tfidf)\n",
    "f1_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score on test (0.9882) is very similar to the one obtained during cross validation for the best hyeraparameter set (f1-score = 0.9861). It implies good generalization. We can also see from the next boxplot, that over the folds, the deviation from the mean score is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'F1-score')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEICAYAAAB1f3LfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLZffv0oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22566bf9668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "metrics_svm = cross_val_score(best_svm, X_train_tfidf, y_train, cv=5)\n",
    "plt.boxplot(metrics_svm)\n",
    "plt.grid()\n",
    "plt.title(\"Boxplot of score per fold\")\n",
    "plt.ylabel(\"F1-score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Compare SVM based spam flter wth another model (Gradient Boosting Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare the previous model, with another model. For that, we choose gradient boosting trees implementation of scikit-learn. Thanks to the ensemble model involved in the GBT, they tend also to give a very good generalization. We find the best hyperparameter set for this model using cross-validation. The consiered hyperparameters are: max depth and number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing and creating classifiert\n",
    "from sklearn import ensemble\n",
    "gbc = ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparameter slist\n",
    "max_depth_list = [2,10,100]\n",
    "n_estimators_list=[5, 50, 100, 200]\n",
    "\n",
    "#creating hyperaparamter list for grid search\n",
    "parameters={'max_depth': max_depth_list, 'n_estimators': n_estimators_list}\n",
    "\n",
    "#creating scorer for f1-score\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "#grid search over the hyperparameter set\n",
    "gbc_grid = GridSearchCV(gbc, parameters, cv=5, scoring=f1_scorer, verbose=1)\n",
    "gbc_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#choosing the best estimator\n",
    "best_gbc= gbc_grid.best_estimator_\n",
    "best_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Mean F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2, 5)</td>\n",
       "      <td>0.932078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2, 50)</td>\n",
       "      <td>0.967502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2, 100)</td>\n",
       "      <td>0.973739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(2, 200)</td>\n",
       "      <td>0.979350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.967793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(10, 50)</td>\n",
       "      <td>0.974688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(10, 100)</td>\n",
       "      <td>0.975831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(10, 200)</td>\n",
       "      <td>0.976863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(100, 5)</td>\n",
       "      <td>0.974640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(100, 50)</td>\n",
       "      <td>0.973663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>0.973792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(100, 200)</td>\n",
       "      <td>0.973658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             C   Mean F1\n",
       "0       (2, 5)  0.932078\n",
       "1      (2, 50)  0.967502\n",
       "2     (2, 100)  0.973739\n",
       "3     (2, 200)  0.979350\n",
       "4      (10, 5)  0.967793\n",
       "5     (10, 50)  0.974688\n",
       "6    (10, 100)  0.975831\n",
       "7    (10, 200)  0.976863\n",
       "8     (100, 5)  0.974640\n",
       "9    (100, 50)  0.973663\n",
       "10  (100, 100)  0.973792\n",
       "11  (100, 200)  0.973658"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_axis = [(i['max_depth'], i['n_estimators']) for i in rf_grid.cv_results_['params']]\n",
    "y_axis = rf_grid.cv_results_['mean_test_score']\n",
    "pd.DataFrame({'C':x_axis, 'Mean F1':y_axis })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98168870803662267"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on new (test) data\n",
    "pred_test = best_rf.predict(X_test_tfidf)\n",
    "f1_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'F1-score')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEICAYAAABiXeIWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2256755dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_gbc = cross_val_score(best_rf, X_train_tfidf, y_train, cv=5)\n",
    "\n",
    "metrics_gbc = cross_val_score(best_rf, X_train_tfidf, y_train, cv=5)\n",
    "plt.boxplot(metrics_rf)\n",
    "plt.grid()\n",
    "plt.title(\"Boxplot of score per fold\")\n",
    "plt.ylabel(\"F1-score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GBC the results are very similar to the ons obtained with SVM (only slighylt lower f1-score in test set). For svm the score on training set was: 0.9882 and for gradient boosting, we obtained 0.9816. However the main difference is on the standar deviation from the results in SVM and GB. We see that the standard deviation of the k-fold results is much lower than for GB. This shows that SVM is able to reach similar resutls for the different sets, therefore it has a better generalization. In fact, this is a well know property of the SVM [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1] Term frequency - inverse document frequency (tf-idf): https://es.m.wikipedia.org/wiki/Tf-idf\n",
    "\n",
    "[2] C. Bishop. _Machine Learning and Pattern Recogtion_\". Springer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
